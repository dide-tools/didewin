---
title: "troubleshooting"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{troubleshooting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- DO NOT EDIT THIS FILE - see vignettes_src and make changes there -->



Base queue object:


```r
ctx <- context::context_save("contexts")
#> [ open:db   ]  rds
#> [ save:id   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ save:name ]  grapy_viceroybutterfly
obj <- didehpc::queue_didehpc(ctx)
#> Loading context 08e8d9dc3cefdd01c31220cd36d1c25d
#> [ context   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ library   ]
#> [ namespace ]
#> [ source    ]
```

# My job has failed

## My job status is `ERROR`

### Caused by an error in your code

If your job status is `ERROR` that *probably* indicates an error in
your code.  There are lots of reasons that this could be for, and
the first challenge is working out what happened.


```r
t <- obj$enqueue(mysimulation(10))
```


```
#> (-) waiting for e519b74...fd7, giving up in 9.5 s (\) waiting for e519b74...fd7,
#> giving up in 8.9 s
```

This job will fail, and `$status()` will report `ERROR`


```r
t$status()
#> [1] "ERROR"
```

The first place to look is the result of the job itself.  Unlike an
error in your console, an error that happens on the cluster can be
returned and inspected:


```r
t$result()
#> <context_task_error in mysimulation(10): could not find function "mysimulation">
```

In this case the error is because the function `mysimulation` does
not exist.

The other place worth looking is the job log

```r
t$log()
#> [ hello     ]  2021-04-08 18:27:55
#> [ wd        ]  Q:/didehpc/20210408-182648
#> [ init      ]  2021-04-08 18:27:55.734
#> [ hostname  ]  FI--DIDECLUST25
#> [ process   ]  1656
#> [ version   ]  0.3.0
#> [ open:db   ]  rds
#> [ context   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ library   ]
#> [ namespace ]
#> [ source    ]
#> [ parallel  ]  running as single core job
#> [ root      ]  Q:\didehpc\20210408-182648\contexts
#> [ context   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ task      ]  e519b74a4b726fd61029766e9c372fd7
#> [ expr      ]  mysimulation(10)
#> [ start     ]  2021-04-08 18:27:55.890
#> [ error     ]
#>     Error in mysimulation(10): could not find function "mysimulation"
#> [ end       ]  2021-04-08 18:27:55.952
#>     Error in context:::main_task_run() : Error while running task:
#>     Execution halted
```

Sometimes there will be additional diagnostic information there.

Here's another example:


```r
t <- obj$enqueue(read.csv("c:/myfile.csv"))
```


```
#> (-) waiting for 784c987...0b7, giving up in 9.5 s (\) waiting for 784c987...0b7,
#> giving up in 9.0 s
```

This job will fail, and `$status()` will report `ERROR`

```r
t$status()
#> [1] "ERROR"
```

Here is the error, which is a bit less informative this time:


```r
t$result()
#> <context_task_error in file(file, "rt"): cannot open the connection>
```

The log gives a better idea of what is going on - the file
`c:/myfile.csv` does not exist (because it is not found on the
cluster; using relative paths is much preferred to absolute paths)


```r
t$log()
#> [ hello     ]  2021-04-08 18:27:57
#> [ wd        ]  Q:/didehpc/20210408-182648
#> [ init      ]  2021-04-08 18:27:57.374
#> [ hostname  ]  FI--DIDECLUST25
#> [ process   ]  2728
#> [ version   ]  0.3.0
#> [ open:db   ]  rds
#> [ context   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ library   ]
#> [ namespace ]
#> [ source    ]
#> [ parallel  ]  running as single core job
#> [ root      ]  Q:\didehpc\20210408-182648\contexts
#> [ context   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ task      ]  784c9873ce4a5771a9b56976e54f50b7
#> [ expr      ]  read.csv("c:/myfile.csv")
#> [ start     ]  2021-04-08 18:27:57.546
#> [ error     ]
#>     Error in file(file, "rt"): cannot open the connection
#> [ end       ]  2021-04-08 18:27:57.609
#>     Error in context:::main_task_run() : Error while running task:
#>     In addition: Warning message:
#>     In file(file, "rt") :
#>       cannot open file 'c:/myfile.csv': No such file or directory
#>     Execution halted
```

The real content of the error message is present in the warning!
You can also get the warnings with


```r
t$result()$warnings
#> [[1]]
#> <simpleWarning in file(file, "rt"): cannot open file 'c:/myfile.csv': No such file or directory>
```

Which will be a list of all warnings generated during the execution
of your task (even if it succeeds).  The traceback also shows what
happened:


```r
t$result()$trace
#>  [1] "context:::main_task_run()"
#>  [2] "task_run(task_id, ctx)"
#>  [3] "eval_safely(dat$expr, dat$envir, \"context_task_error\", 3)"
#>  [4] "tryCatch(withCallingHandlers(eval(expr, envir), warning = function(e) warni"
#>  [5] "tryCatchList(expr, classes, parentenv, handlers)"
#>  [6] "tryCatchOne(expr, names, parentenv, handlers[[1]])"
#>  [7] "doTryCatch(return(expr), name, parentenv, handler)"
#>  [8] "withCallingHandlers(eval(expr, envir), warning = function(e) warnings$add(e"
#>  [9] "eval(expr, envir)"
#> [10] "eval(expr, envir)"
#> [11] "read.csv(\"c:/myfile.csv\")"
#> [12] "read.table(file = file, header = header, sep = sep, quote = quote, dec = de"
#> [13] "file(file, \"rt\")"
```

### Caused by an error during startup

These are harder to troubleshoot but we can still pull some
information out.  The example here was a real-world case and
illustrates one of the issues with using a shared filesystem in the
way that we do here.



Suppose you have a context that uses some code in `mycode.R`:

```r
times2 <- function(x) {
  2 * x
}
```

You create a connection to the cluster:


```r
ctx <- context::context_save("contexts", sources = "mycode.R")
#> [ open:db   ]  rds
#> [ save:id   ]  73f7926735df1c5aaa03a7cef4c054ef
#> [ save:name ]  transcrystalline_bushsqueaker
obj <- didehpc::queue_didehpc(ctx)
#> Loading context 73f7926735df1c5aaa03a7cef4c054ef
#> [ context   ]  73f7926735df1c5aaa03a7cef4c054ef
#> [ library   ]
#> [ namespace ]
#> [ source    ]  mycode.R
```

Everything seems to work fine:


```r
t <- obj$enqueue(times2(10))
t$wait(10)
#> (-) waiting for e567b01...7b9, giving up in 9.5 s (\) waiting for e567b01...7b9,
#> giving up in 9.0 s
#> [1] 20
```

...but then you're editing the file and save the file but it is not
syntactically correct:


```r
times2 <- function(x) {
  2 * x
}
newfun <- function(x)
```

And then you either submit a job, **or** a job that you have
previously submitted gets run (which could happen ages after you
submit it if the cluster is busy).


```r
t <- obj$enqueue(times2(10))
t$wait(10)
#> (-) waiting for 3eba909...d5d, giving up in 9.5 s (\) waiting for 3eba909...d5d,
#> giving up in 9.0 s
#> <context_task_error in source(s, envir): mycode.R:5:0: unexpected end of input
#> 3: }
#> 4: newfun <- function(x)
#>   ^>
t$status()
#> [1] "ERROR"
```

The error here has happened before getting to your code - it is
happening when context loads the source files.  The log makes this
a bit clearer:


```r
t$log()
#> [ hello     ]  2021-04-08 18:28:00
#> [ wd        ]  Q:/didehpc/20210408-182648
#> [ init      ]  2021-04-08 18:28:00.859
#> [ hostname  ]  FI--DIDECLUST25
#> [ process   ]  804
#> [ version   ]  0.3.0
#> [ open:db   ]  rds
#> [ context   ]  73f7926735df1c5aaa03a7cef4c054ef
#> [ library   ]
#> [ namespace ]
#> [ source    ]  mycode.R
#>     Error in source(s, envir) : mycode.R:5:0: unexpected end of input
#>     3: }
#>     4: newfun <- function(x)
#>       ^
#>     Calls: <Anonymous> -> withCallingHandlers -> context_load -> source
#>     Execution halted
```

### My jobs are getting stuck at `PENDING`

This is the most annoying one, and can happen for many reasons.
You can see via the [web interface](mrcdata.dide.ic.ac.uk/hpc/) or
the Microsoft cluster tools that your job has failed but `didehpc`
is reporting it as pending.  This happens when something has failed
during the script that runs *before* any `didehpc` code runs on the
cluster.

Things that have triggered this situation in the past:

* An error in the Microsoft cluster tools
* A misconfigured node (sometimes they are missing particular software)
* A networking issue
* Gremlins
* Network path mapping error

There are doubtless others.  Here, I'll simulate one so you can see
how to troubleshoot it.  I'm going to *deliberately* misconfigure
the network share that this is running on so that the cluster will
not be able to map it and the job will fail to start


```r
home <- didehpc::path_mapping("home", getwd(),
                              "//fi--wronghost/path", "Q:")
```

The host `fi--wronghost` does not exist so things will likely fail
on startup.


```r
config <- didehpc::didehpc_config(home = home)
ctx <- context::context_save("contexts")
#> [ open:db   ]  rds
obj <- didehpc::queue_didehpc(ctx, config)
#> Loading context 08e8d9dc3cefdd01c31220cd36d1c25d
#> [ context   ]  08e8d9dc3cefdd01c31220cd36d1c25d
#> [ library   ]
#> [ namespace ]
#> [ source    ]
```

Submit a job:

```r
t <- obj$enqueue(sessionInfo())
```

And wait...


```r
t$wait(10)
#> (-) waiting for 390cbe8...9bb, giving up in 9.5 s (\) waiting for 390cbe8...9bb,
#> giving up in 9.0 s (|) waiting for 390cbe8...9bb, giving up in 8.5 s (/) waiting
#> for 390cbe8...9bb, giving up in 8.0 s (-) waiting for 390cbe8...9bb, giving
#> up in 7.5 s (\) waiting for 390cbe8...9bb, giving up in 6.9 s (|) waiting for
#> 390cbe8...9bb, giving up in 6.4 s (/) waiting for 390cbe8...9bb, giving up
#> in 5.9 s (-) waiting for 390cbe8...9bb, giving up in 5.4 s (\) waiting for
#> 390cbe8...9bb, giving up in 4.9 s (|) waiting for 390cbe8...9bb, giving up
#> in 4.4 s (/) waiting for 390cbe8...9bb, giving up in 3.9 s (-) waiting for
#> 390cbe8...9bb, giving up in 3.4 s (\) waiting for 390cbe8...9bb, giving up
#> in 2.9 s (|) waiting for 390cbe8...9bb, giving up in 2.4 s (/) waiting for
#> 390cbe8...9bb, giving up in 1.9 s (-) waiting for 390cbe8...9bb, giving up
#> in 1.3 s (\) waiting for 390cbe8...9bb, giving up in 0.8 s (|) waiting for
#> 390cbe8...9bb, giving up in 0.3 s (/) waiting for 390cbe8...9bb, giving up in
#> 0.0 s
#> Error in task_wait(self$root$db, self$id, timeout, time_poll, progress): task not returned in time
```

It's never going to succeed and yet it's status will stay as `PENDING`:


```r
t$status()
#> [1] "PENDING"
```

To get the log from the DIDE cluster you can run:


```r
obj$dide_log(t)
#> [1] "Task failed during execution with exit code . Please check task's output for error details."
#> [2] "Output                          : The network path was not found."
```

which here indicates that the network path was not found (because
it was wrong!)

You can also update any incorrect statuses by running:


```r
obj$reconcile()
#> Fetching job status from the cluster...
#>   ...done
#> manually erroring task 390cbe84b7a4390dde03a793d8b739bb
#> Tasks have failed while context booting:
#>   - 390cbe84b7a4390dde03a793d8b739bb
```

Which will print information about anything that was adjusted.
